!pip install xgboost
 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.svm import SVR
from sklearn.linear_model import Ridge, Lasso
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score
from sklearn import metrics
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingRegressor, RandomForestRegressor, StackingRegressor
import warnings
warnings.filterwarnings('ignore')
from sklearn.tree import DecisionTreeClassifier, plot_tree
 
import graphviz
import xgboost as xgb
from xgboost import XGBRegressor
 
from sklearn.datasets import fetch_california_housing
 
df = pd.read_csv("C:/Users/kavan/Downloads/archive (7)/Indian Liver Patient Dataset (ILPD).csv")
df.head()
 
df.isnull().sum()
 
df1 = df.dropna()
df1.isnull().any()
 
numeric_df = df1.select_dtypes(include=['number'])
 
 
fig, ax = plt.subplots(figsize=(7, 7))
sns.heatmap(abs(numeric_df.corr()), annot=True, square=True, cbar=False, ax=ax, linewidths=0.25)
plt.show()
 
df2 = df1.drop(columns= ['direct_bilirubin', 'albumin', 'sgpt'])
 
df2['is_patient'] = df2['is_patient'].replace(1,0)
df2['is_patient'] = df2['is_patient'].replace(2,1)
 
print('How many people have disease:', '\n', df2.groupby('gender')[['is_patient']].sum(), '\n')
print('How many people participated in the study:', '\n', df2.groupby('gender')[['is_patient']].count())
 
print('Percentage of people with the disease depending on gender:')
df2.groupby('gender')[['is_patient']].sum()/ df2.groupby('gender')[['is_patient']].count()
 
X = df2[['gender', 'tot_bilirubin','tot_proteins','ag_ratio','sgot','alkphos']]
y = pd.Series(df2['is_patient'])
 
labelencoder = LabelEncoder()
X['gender'] = labelencoder.fit_transform(X['gender'])
 
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(x_train)
X_test = scaler.transform(x_test)
 
ADB = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),
                         n_estimators=125,
                         learning_rate = 0.6,
                         random_state=42)
 
ADB.fit(X_train, y_train)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
 
n_scores = cross_val_score(ADB, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
('Accuracy: %.3f' % (np.mean(n_scores)*100))
 
labels = ADB.predict(X_test)
matrix = metrics.confusion_matrix(y_test, labels)
# creating a heat map to visualize confusion matrix
sns.heatmap(matrix.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');
 
logit_roc_auc = metrics.roc_auc_score(y_test, labels)
fpr, tpr, thresholds = metrics.roc_curve(y_test, ADB.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='(area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

#stacking algorithm
xgb = XGBRegressor()
rf = RandomForestRegressor(n_estimators=400, max_depth=5, max_features=6)
ridge = Ridge()
lasso = Lasso()
svr = SVR(kernel='rbf')
import xgboost as xgb
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge, Lasso
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
data = fetch_california_housing()
X = data.data
y = data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
ridge = Ridge()
svr = SVR()
rf = RandomForestRegressor()
lasso = Lasso()
estimators = [('ridge', ridge), ('svr', svr), ('rf', rf), ('lasso', lasso)]
reg = StackingRegressor(estimators=estimators, final_estimator=xgb.XGBRegressor(objective="reg:squarederror"))
reg.fit(X_train, y_train)
pred = reg.predict(X_test)
xgb.plot_importance(reg.final_estimator_)
plt.rcParams['figure.figsize'] = [5, 5]
plt.show()
score = metrics.r2_score(y_test, pred)
print(score)
